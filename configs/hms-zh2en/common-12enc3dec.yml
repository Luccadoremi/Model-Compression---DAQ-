train-subtransformer: True
quant-noise-pq: 0.1 
quant-noise-pq-block-size: 8
# model
arch: transformersuper_wmt_en_fr
share-all-embeddings: True
max-tokens: 6144
data: data/binary/hms_zh_en

# training settings
optimizer: adam
adam-betas: (0.9, 0.999)
clip-norm: 0.0
weight-decay: 0.0
dropout: 0.1
attention-dropout: 0.1
criterion: label_smoothed_cross_entropy
label-smoothing: 0.1
left-pad-source: False

ddp-backend: no_c10d
fp16: True
reset-optimizer: True

max-update: 200000
warmup-updates: 4000
lr-scheduler: cosine
warmup-init-lr: 1e-12
max-lr: 0.0006
lr: 1e-12
lr-shrink: 1

# logging
keep-last-epochs: 30
#save-interval: 1
save-interval-updates: 5000
#validate-interval: 1

# SuperTransformer configs

# We train the SubTransformer inside the SuperTransformer, so need to specify a SuperTransformer
# From algorithm side, we can train a totally standalone SubTransformer and it is unnecessary to specify a SuperTransformer
# However, from implementation side, it is easier to do a Subtransformer training by always sampling the same desired SubTransformer from a specified SuperTransformer

encoder-embed-dim: 512
decoder-embed-dim: 512

encoder-ffn-embed-dim: 1024
decoder-ffn-embed-dim: 1024

encoder-layers: 12
decoder-layers: 3

encoder-attention-heads: 4
decoder-attention-heads: 4

qkv-dim: 512
