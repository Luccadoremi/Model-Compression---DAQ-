train-subtransformer: True

restore-file: checkpoints/iwslt14.de-en/our-no_noise/checkpoint_best.pt
quantization-config-path: configs/iwslt14.de-en/subtransformer/pq-quantization-n5-bucket32.yml
# model
arch: transformersuper_iwslt_de_en
max-tokens: 4096
data: data/binary/iwslt14_de_en
source-lang: de
target-lang: en

# training settings
optimizer: adam
adam-betas: (0.9, 0.98)
weight-decay: 0.0001
dropout: 0.375
criterion: label_smoothed_cross_entropy
label-smoothing: 0.1

fp16: True
ddp-backend: no_c10d
reset-optimizer: True
reset-dataloader: True
reset-meters: True
no-save-optimizer-state: True  #to reduce model checkpoint size

# warmup from warmup-init-lr to lr (warmup-updates steps); then inverse sqrt anneal (max-update - warmup-updates steps)
max-epoch: 12
warmup-updates: 4000
lr-scheduler: inverse_sqrt
warmup-init-lr: 1e-7
lr: 0.0005

# logging
keep-last-epochs: 20
save-interval: 1
validate-interval: 1

# SuperTransformer configs

# We train the SubTransformer inside the SuperTransformer, so need to specify a SuperTransformer
# From algorithm side, we can train a totally standalone SubTransformer and it is unnecessary to specify a SuperTransformer
# However, from implementation side, it is easier to do a Subtransformer training by always sampling the same desired SubTransformer from a specified SuperTransformer

encoder-embed-dim: 640
decoder-embed-dim: 640

encoder-ffn-embed-dim: 2048
decoder-ffn-embed-dim: 2048

encoder-layers: 6
decoder-layers: 6

encoder-attention-heads: 4
decoder-attention-heads: 4

qkv-dim: 512
